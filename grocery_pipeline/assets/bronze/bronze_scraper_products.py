"""
Bronze scraper assets: one per retailer, generated by factory.

Each asset calls run_scrape() from the POC scraper module, transforms the
Product list into BigQuery rows, and batch-inserts to
grocery-pipe-line.grocery_bronze.bronze_scraper_products.
"""

import sys
from datetime import datetime, timezone
from pathlib import Path

from dagster import (
    asset,
    AssetExecutionContext,
    RetryPolicy,
)
from google.cloud import bigquery

from grocery_pipeline.assets.bronze.scraper_config import (
    ScraperConfig,
    BQ_PROJECT,
    TABLE_ID,
    products_to_bq_rows,
    flush_rows_to_bq,
)

# ---------------------------------------------------------------------------
# sys.path injection so we can import POC scraper modules
# ---------------------------------------------------------------------------

_POC_SRC = str(Path.home() / "lab" / "retail_scraper_poc" / "src")
if _POC_SRC not in sys.path:
    sys.path.insert(0, _POC_SRC)

# Validate imports are possible at module load time
try:
    from poc_heb_single_store import run_scrape as _heb_run  # noqa: F401
    from poc_meijer_single_store import run_scrape as _meijer_run  # noqa: F401
    from poc_walmart_single_store import run_scrape as _walmart_run  # noqa: F401
    from poc_vons_single_store import run_scrape as _vons_run  # noqa: F401
    from poc_wd_single_store import run_scrape as _wd_run  # noqa: F401
    from poc_sprouts_single_store import run_scrape as _sprouts_run  # noqa: F401
except ImportError as e:
    raise ImportError(
        f"Could not import POC scraper modules from {_POC_SRC}. "
        f"Ensure retail_scraper_poc repo is present: {e}"
    ) from e

# ---------------------------------------------------------------------------
# Default terms file
# ---------------------------------------------------------------------------

_DEFAULT_TERMS_FILE = str(
    Path.home()
    / "pipelines"
    / "grocery_pipeline"
    / "grocery_pipeline"
    / "config"
    / "canonical_search_terms.yaml"
)

# ---------------------------------------------------------------------------
# Retailer registry
# ---------------------------------------------------------------------------

RETAILER_REGISTRY = {
    "heb": {
        "module": "poc_heb_single_store",
        "run_scrape": _heb_run,
        "connection": "cdp",
        "default_cdp_port": 9224,
        "default_zip": "78704",
    },
    "meijer": {
        "module": "poc_meijer_single_store",
        "run_scrape": _meijer_run,
        "connection": "cdp",
        "default_cdp_port": 9225,
        "default_zip": "49503",
    },
    "walmart": {
        "module": "poc_walmart_single_store",
        "run_scrape": _walmart_run,
        "connection": "cdp",
        "default_cdp_port": 9226,
        "default_zip": "33019",
    },
    "vons": {
        "module": "poc_vons_single_store",
        "run_scrape": _vons_run,
        "connection": "cdp",
        "default_cdp_port": 9223,
        "default_zip": "92123",
    },
    "wd": {
        "module": "poc_wd_single_store",
        "run_scrape": _wd_run,
        "connection": "cdp",
        "default_cdp_port": 9222,
        "default_zip": "33009",
    },
    "sprouts": {
        "module": "poc_sprouts_single_store",
        "run_scrape": _sprouts_run,
        "connection": "launch",
        "default_cdp_port": None,
        "default_zip": "92123",
    },
}


# ---------------------------------------------------------------------------
# Asset factory
# ---------------------------------------------------------------------------


def build_scraper_assets():
    """Generate one Dagster asset per retailer from the registry."""
    assets = []

    for retailer, info in RETAILER_REGISTRY.items():

        def _make_asset(retailer=retailer, info=info):
            @asset(
                name=f"bronze_{retailer}_products_daily",
                compute_kind="playwright",
                retry_policy=RetryPolicy(max_retries=2, delay=300),
                op_tags={"dagster/max_runtime": 3600},
            )
            def _asset_fn(context: AssetExecutionContext, config: ScraperConfig):
                run_fn = info["run_scrape"]
                connection = info["connection"]

                # Build config dict for run_scrape()
                terms_file = config.terms_file or _DEFAULT_TERMS_FILE
                scrape_config = {
                    "zipcode": config.zipcode,
                    "terms_file": terms_file,
                    "max_per_query": config.max_per_query,
                    "delay_min": config.delay_min,
                    "delay_max": config.delay_max,
                    "page_timeout": config.page_timeout,
                    "settle": config.settle,
                    "scrape_wait": config.scrape_wait,
                    "manual_gate": config.manual_gate,
                    "manual_token": config.manual_token,
                }

                # CDP connection check (skip for Sprouts which launches its own browser)
                if connection == "cdp":
                    cdp_url = config.cdp_url
                    if not cdp_url:
                        port = info["default_cdp_port"]
                        cdp_url = f"http://127.0.0.1:{port}"
                    scrape_config["cdp_url"] = cdp_url

                    # Fail fast if CDP is unreachable
                    import urllib.request
                    import urllib.error
                    try:
                        urllib.request.urlopen(f"{cdp_url}/json", timeout=5)
                    except (urllib.error.URLError, OSError) as e:
                        raise RuntimeError(
                            f"CDP endpoint unreachable at {cdp_url}. "
                            f"Run launch_sessions.sh first: {e}"
                        ) from e
                    context.log.info(f"CDP endpoint reachable: {cdp_url}")
                else:
                    # Sprouts: no CDP, uses headful config
                    scrape_config["headful"] = config.headful

                context.log.info(
                    f"Starting {retailer} scrape | zip={config.zipcode} "
                    f"terms={terms_file}"
                )

                # Run the scraper
                products = run_fn(scrape_config)
                context.log.info(
                    f"{retailer} scrape complete: {len(products)} products"
                )

                if not products:
                    context.log.warning(f"No products returned for {retailer}")
                    return

                # Transform and insert to BigQuery
                snapshot_date = datetime.now(timezone.utc).date().isoformat()
                ingestion_ts = datetime.now(timezone.utc).isoformat()

                rows = products_to_bq_rows(products, snapshot_date, ingestion_ts)

                client = bigquery.Client(project=BQ_PROJECT)
                total = flush_rows_to_bq(client, rows, context)

                context.log.info(
                    f"{retailer} ingestion complete: {total} rows -> {TABLE_ID}"
                )

            return _asset_fn

        assets.append(_make_asset())

    return assets


# Module-level list of all scraper assets
scraper_assets = build_scraper_assets()
